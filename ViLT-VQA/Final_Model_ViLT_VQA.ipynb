{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6GdkLCpG3Il"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u3lYgPHHAGm"
      },
      "outputs": [],
      "source": [
        "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fskl2MIIQ0P"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import shutil\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet152, ResNet152_Weights\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXc5y1rxHVQ1"
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/zips/val2014.zip\n",
        "!unzip /content/val2014.zip\n",
        "!rm /content/val2014.zip\n",
        "\n",
        "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip\n",
        "!unzip /content/v2_Questions_Val_mscoco.zip\n",
        "!rm /content/v2_Questions_Val_mscoco.zip\n",
        "!mv /content/v2_OpenEnded_mscoco_val2014_questions.json /content/val2014questions.json\n",
        "\n",
        "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip\n",
        "!unzip /content/v2_Annotations_Val_mscoco.zip\n",
        "!rm /content/v2_Annotations_Val_mscoco.zip\n",
        "!mv /content/v2_mscoco_val2014_annotations.json /content/val2014answers.json\n",
        "\n",
        "!mkdir /content/questions\n",
        "!mkdir /content/answers\n",
        "\n",
        "!mv /content/val2014questions.json /content/questions/val.json\n",
        "!mv /content/val2014answers.json /content/answers/val.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toa65xbrH40Y"
      },
      "outputs": [],
      "source": [
        "class VQADataset(Dataset):\n",
        "    def __init__(self, phase, questions_dir, answers_dir, images_dir):\n",
        "        self.phase = phase\n",
        "        self.questions_json = questions_dir + \"/\" + self.phase + \".json\"\n",
        "        self.answers_json = answers_dir + \"/\" + self.phase + \".json\"\n",
        "        self.images_dir = images_dir\n",
        "\n",
        "        self.dataset = self.create_dataset()\n",
        "\n",
        "\n",
        "    def create_dataset(self):\n",
        "        with open(self.questions_json) as f:\n",
        "            questions = json.load(f)[\"questions\"]\n",
        "        with open(self.answers_json) as f:\n",
        "            answers = json.load(f)[\"annotations\"]\n",
        "\n",
        "        dataset = []\n",
        "        file_loop = tqdm(enumerate(zip(questions, answers)), total=len(questions), colour=\"green\")\n",
        "        for idx, (q, a) in file_loop:\n",
        "            if(q[\"image_id\"]!=a[\"image_id\"]):\n",
        "                continue\n",
        "            image_id = str(q[\"image_id\"])\n",
        "            image_path = self.images_dir + \"/\" + self.phase + \"/\" + image_id + \".jpg\"\n",
        "\n",
        "            ans = a[\"answers\"]\n",
        "            answers = []\n",
        "\n",
        "            for answer in ans:\n",
        "                if((answer[\"answer_confidence\"]==\"yes\") and (answer[\"answer\"] not in answers)):\n",
        "                    answers.append(answer[\"answer\"].lower())\n",
        "\n",
        "            sample = {}\n",
        "            sample[\"image_path\"] = image_path\n",
        "            sample[\"question\"] = q[\"question\"]\n",
        "            sample[\"answers\"] = answers\n",
        "            dataset.append(sample)\n",
        "\n",
        "            file_loop.set_description(f\"Generating {self.phase} data\")\n",
        "\n",
        "        random.shuffle(dataset)\n",
        "        return dataset\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "\n",
        "        sample = self.dataset[index]\n",
        "        image_path =  sample[\"image_path\"]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        question = sample[\"question\"]\n",
        "        answers = sample[\"answers\"]\n",
        "\n",
        "        return image, question, answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcfE1rzIOqVZ"
      },
      "outputs": [],
      "source": [
        "def rename_image_dataset(phase, input_dir, num_samples=None):\n",
        "    images = os.listdir(input_dir)\n",
        "    if(len(images)==0):\n",
        "        print(\"Input directory {} is empty\".format(input_dir))\n",
        "    else:\n",
        "        if num_samples is not None:\n",
        "            random.shuffle(images)\n",
        "            images = images[:num_samples]\n",
        "        image_count = len(images)\n",
        "        file_loop = tqdm(enumerate(images), total=len(images), colour=\"green\")\n",
        "        for n_image, image_name in file_loop:\n",
        "            try:\n",
        "                input_image_path = os.path.join(input_dir + '/', image_name)\n",
        "                with open(input_image_path, 'r+b') as f:\n",
        "                    with Image.open(f) as img:\n",
        "                        image_name = image_name.split(\"_\")[-1].lstrip(\"0\")\n",
        "                        output_image_path = os.path.join(input_dir + '/', image_name)\n",
        "                        img.save(output_image_path, img.format)\n",
        "                        os.remove(input_image_path)\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(\"Error while resizing {}\".format(image_name))\n",
        "                pass\n",
        "            file_loop.set_description(f\"Resizing {phase} images...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1uOyVcaPRgb"
      },
      "outputs": [],
      "source": [
        "rename_image_dataset(phase=\"val\", input_dir=\"/content/val2014\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihAMJmpSKBUz"
      },
      "outputs": [],
      "source": [
        "val_dataset = VQADataset(phase=\"val\",\n",
        "                         questions_dir=\"/content/questions\",\n",
        "                         answers_dir=\"/content/answers\",\n",
        "                         images_dir=\"/content/val2014\")\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=1,\n",
        "                        shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JL_fEXkK2pH"
      },
      "outputs": [],
      "source": [
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A8tVaOSb-4e"
      },
      "outputs": [],
      "source": [
        "# device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "device = 'cpu'\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTDxMug1cIHg"
      },
      "outputs": [],
      "source": [
        "model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RspTBEJ-rRIq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0Kv26uPrIdh"
      },
      "outputs": [],
      "source": [
        "# Saving model checkpoint in google drive.\n",
        "\n",
        "import torch\n",
        "\n",
        "# torch.save(model.state_dict(), '/content/drive/MyDrive/model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSHk-6mNr4a3"
      },
      "outputs": [],
      "source": [
        "# Loading model from google drive.\n",
        "\n",
        "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(model, open('model3.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "hGeDeE7kceBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "BIwlv0uoWT9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERPqL3lHK5lF"
      },
      "outputs": [],
      "source": [
        "questions_json = \"/content/questions/val.json\"\n",
        "answers_json = \"/content/answers/val.json\"\n",
        "with open(questions_json) as f:\n",
        "    questions = json.load(f)[\"questions\"]\n",
        "with open(answers_json) as f:\n",
        "    answers = json.load(f)[\"annotations\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxOSf2BtXnya"
      },
      "outputs": [],
      "source": [
        "questions[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWEaBjL1Xq8g"
      },
      "outputs": [],
      "source": [
        "answers[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference on custom images and questions"
      ],
      "metadata": {
        "id": "dlHiGvNUbk3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(im_path, ques):\n",
        "  image = Image.open(im_path).convert(\"RGB\")\n",
        "  question = ques\n",
        "\n",
        "  encodings = processor(image, question, return_tensors=\"pt\").to(device)\n",
        "  outputs = model(**encodings)\n",
        "  logits = outputs.logits\n",
        "  _, answer_index_top5 = torch.topk(logits, 5)\n",
        "\n",
        "  predicted_answer = []\n",
        "  for pred_answer_index in answer_index_top5[0, :]:\n",
        "      predicted_answer.append(model.config.id2label[pred_answer_index.item()])\n",
        "\n",
        "  return predicted_answer[0]"
      ],
      "metadata": {
        "id": "BzsV1ysajA-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"/content/testimage.jpg\").convert(\"RGB\")\n",
        "question = \"What object is in the image?\"\n",
        "all_answers = []\n",
        "\n",
        "encodings = processor(image, question, return_tensors=\"pt\").to(device)\n",
        "outputs = model(**encodings)\n",
        "logits = outputs.logits\n",
        "_, answer_index_top5 = torch.topk(logits, 5)\n",
        "\n",
        "predicted_answer = []\n",
        "for pred_answer_index in answer_index_top5[0, :]:\n",
        "    predicted_answer.append(model.config.id2label[pred_answer_index.item()])"
      ],
      "metadata": {
        "id": "SSLVnpZ9ZNlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_answer)"
      ],
      "metadata": {
        "id": "iXsFeKi2bdiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating validation accuracy."
      ],
      "metadata": {
        "id": "Q8Sw1mN0brLg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COENYc3MV8zR"
      },
      "outputs": [],
      "source": [
        "images_dir=\"/content/val2014\"\n",
        "\n",
        "questions = questions[2:]\n",
        "answers = answers[2:]\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "correct_q = 0\n",
        "total_q = 0\n",
        "\n",
        "file_loop = tqdm(enumerate(zip(questions, answers)), total=len(questions), colour=\"green\")\n",
        "for idx, (q, a) in file_loop:\n",
        "    if(q[\"image_id\"]!=a[\"image_id\"]):\n",
        "        continue\n",
        "    image_id = str(q[\"image_id\"])\n",
        "    image_path = images_dir + \"/\" + image_id + \".jpg\"\n",
        "\n",
        "    ans = a[\"answers\"]\n",
        "    all_answers = []\n",
        "\n",
        "    for answer in ans:\n",
        "        if((answer[\"answer_confidence\"]==\"yes\") and (answer[\"answer\"] not in all_answers)):\n",
        "            all_answers.append(answer[\"answer\"].lower())\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    question = q[\"question\"]\n",
        "    all_answers = all_answers\n",
        "\n",
        "    encodings = processor(image, question, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**encodings)\n",
        "    logits = outputs.logits\n",
        "    _, answer_index_top5 = torch.topk(logits, 5)\n",
        "    predicted_answer = []\n",
        "    for pred_answer_index in answer_index_top5[0, :]:\n",
        "        predicted_answer.append(model.config.id2label[pred_answer_index.item()])\n",
        "\n",
        "    if predicted_answer[0] in all_answers:\n",
        "        correct_q += 1\n",
        "        total_q += 1\n",
        "    else:\n",
        "        total_q += 1\n",
        "\n",
        "    for ans in all_answers:\n",
        "        total += 1\n",
        "        if ans in predicted_answer:\n",
        "            correct+=1\n",
        "\n",
        "    file_loop.set_description(f\"Testing on validation data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeaFDqowmDQf"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy :\", correct_q/total_q *100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx7I5zT0wAY2"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy :\", correct/total *100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}