{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfEXR5v4uwlL"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import shutil\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet152, ResNet152_Weights\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSYDXAUlUcSu"
      },
      "outputs": [],
      "source": [
        "#removing unnecessary files from colab\n",
        "!rm -rf /content/sample_data\n",
        "\n",
        "#for downloading training data\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "!unzip /content/train2014.zip\n",
        "!rm /content/train2014.zip\n",
        "\n",
        "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip\n",
        "!unzip /content/v2_Questions_Train_mscoco.zip\n",
        "!rm /content/v2_Questions_Train_mscoco.zip\n",
        "!mv /content/v2_OpenEnded_mscoco_train2014_questions.json /content/train2014questions.json\n",
        "\n",
        "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip\n",
        "!unzip /content/v2_Annotations_Train_mscoco.zip\n",
        "!rm /content/v2_Annotations_Train_mscoco.zip\n",
        "!mv /content/v2_mscoco_train2014_annotations.json /content/train2014answers.json\n",
        "\n",
        "\n",
        "# for downloading validation data\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip\n",
        "!unzip /content/val2014.zip\n",
        "!rm /content/val2014.zip\n",
        "\n",
        "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip\n",
        "!unzip /content/v2_Questions_Val_mscoco.zip\n",
        "!rm /content/v2_Questions_Val_mscoco.zip\n",
        "!mv /content/v2_OpenEnded_mscoco_val2014_questions.json /content/val2014questions.json\n",
        "\n",
        "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip\n",
        "!unzip /content/v2_Annotations_Val_mscoco.zip\n",
        "!rm /content/v2_Annotations_Val_mscoco.zip\n",
        "!mv /content/v2_mscoco_val2014_annotations.json /content/val2014answers.json\n",
        "\n",
        "clear_output()\n",
        "\n",
        "!mkdir /content/questions\n",
        "!mkdir /content/answers\n",
        "\n",
        "!mv /content/train2014questions.json /content/questions/train.json\n",
        "!mv /content/val2014questions.json /content/questions/val.json\n",
        "!mv /content/train2014answers.json /content/answers/train.json\n",
        "!mv /content/val2014answers.json /content/answers/val.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgov2mz0vOXN"
      },
      "outputs": [],
      "source": [
        "def resize_image(image, image_size):\n",
        "    return image.resize(image_size, Image.LANCZOS)\n",
        "\n",
        "def resize_image_dataset(phase, input_dir, images_dir, image_size, num_samples=None):\n",
        "    images = os.listdir(input_dir)\n",
        "    if(len(images)==0):\n",
        "        print(\"Input directory {} is empty\".format(input_dir))\n",
        "    else:\n",
        "        output_dir = images_dir + \"/\" + phase\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        if num_samples is not None:\n",
        "            random.shuffle(images)\n",
        "            images = images[:num_samples]\n",
        "        image_count = len(images)\n",
        "        file_loop = tqdm(enumerate(images), total=len(images), colour=\"green\")\n",
        "        for n_image, image_name in file_loop:\n",
        "            try:\n",
        "                with open(os.path.join(input_dir + '/', image_name), 'r+b') as f:\n",
        "                    with Image.open(f) as img:\n",
        "                        img = resize_image(img, image_size)\n",
        "                        image_name = image_name.split(\"_\")[-1].lstrip(\"0\")\n",
        "                        output_image_path = os.path.join(output_dir + '/', image_name)\n",
        "                        img.save(output_image_path, img.format)\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(\"Error while resizing {}\".format(image_name))\n",
        "                pass\n",
        "            file_loop.set_description(f\"Resizing {phase} images...\")\n",
        "        shutil.rmtree(input_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXZGViBUu3p-"
      },
      "outputs": [],
      "source": [
        "def load_text_file(file_path):\n",
        "    with open(file_path) as f:\n",
        "        text = f.read().splitlines()\n",
        "    return text\n",
        "\n",
        "\n",
        "SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
        "def tokenise(sentence):\n",
        "    tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n",
        "    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, vocab_file_path):\n",
        "        self.vocab = load_text_file(vocab_file_path)\n",
        "        self.word2index = {word:index for index, word in enumerate(self.vocab)}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.unk_index = self.word2index[\"<unk>\"] if \"<unk>\" in self.word2index else None\n",
        "    \n",
        "\n",
        "    def vocabSize(self):\n",
        "        return self.vocab_size\n",
        "    \n",
        "\n",
        "    def is_present(self, word):\n",
        "        if word in self.vocab:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    def idx2word(self, idx):\n",
        "        return self.vocab[idx]\n",
        "    \n",
        "\n",
        "    def word2idx(self, word):\n",
        "        if word in self.word2index:\n",
        "            return self.word2index[word]\n",
        "        elif self.unk_index is not None:\n",
        "            return self.unk_index\n",
        "        else:\n",
        "            raise ValueError(\"word {} is not in dictionary and <unk> does not exist\".format(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prLqXm0_gUco"
      },
      "outputs": [],
      "source": [
        "def create_question_vocab(json_file, vocab_dir, min_word_count=2):\n",
        "    with open(json_file) as f:\n",
        "        data = json.load(f)[\"questions\"]\n",
        "\n",
        "    vocab = []\n",
        "    data_loop = tqdm(enumerate(data), total=len(data), colour=\"green\")\n",
        "    for idx, data in data_loop:\n",
        "        vocab += tokenise(data[\"question\"])\n",
        "\n",
        "        data_loop.set_description(\"Generating Question Vocabulary\")\n",
        "\n",
        "    word_count = Counter(vocab)\n",
        "    vocab.clear()\n",
        "\n",
        "    for word in word_count:\n",
        "        if word_count[word]>min_word_count:\n",
        "            vocab.append(word)\n",
        "\n",
        "    vocab.sort()\n",
        "    vocab.insert(0, \"<pad>\")\n",
        "    vocab.insert(1, \"<unk>\")   \n",
        "    \n",
        "    if not os.path.exists(vocab_dir):\n",
        "        os.makedirs(vocab_dir)\n",
        "    \n",
        "    question_vocab_file = vocab_dir + \"/question_vocab.txt\"\n",
        "    with open(question_vocab_file, \"w\") as f:\n",
        "        f.writelines([word+\"\\n\" for word in vocab])\n",
        "    \n",
        "    return question_vocab_file, len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROGxbEdB10q_"
      },
      "outputs": [],
      "source": [
        "def create_answer_vocab(json_file, vocab_dir, topN=None):\n",
        "    with open(json_file) as f:\n",
        "        data = json.load(f)[\"annotations\"]\n",
        "\n",
        "    vocab = []\n",
        "    data_loop = tqdm(enumerate(data), total=len(data), colour=\"green\")\n",
        "    for idx, data in data_loop:\n",
        "        answers = data[\"answers\"]\n",
        "        for answer in answers:\n",
        "            if (answer[\"answer_confidence\"]==\"yes\"):\n",
        "                vocab += tokenise(answer[\"answer\"])\n",
        "        data_loop.set_description(\"Generating Answer Vocabulary\")\n",
        "\n",
        "    word_count = Counter(vocab)\n",
        "    \n",
        "    vocab.clear()\n",
        "    for word in word_count:\n",
        "        vocab.append(word)\n",
        "\n",
        "    if (topN is not None) and (len(vocab)>topN):\n",
        "        vocab = vocab[:topN]\n",
        "    vocab.insert(0, \"<unk>\")\n",
        "\n",
        "    if not os.path.exists(vocab_dir):\n",
        "        os.makedirs(vocab_dir)\n",
        "    \n",
        "    answer_vocab_file = vocab_dir + \"/answer_vocab.txt\"\n",
        "    with open(answer_vocab_file, \"w\") as f:\n",
        "        f.writelines([word+\"\\n\" for word in vocab])\n",
        "    \n",
        "    return answer_vocab_file, len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOXGjHOYIeuW"
      },
      "outputs": [],
      "source": [
        "class VQADataset(Dataset):\n",
        "    def __init__(self, phase, questions_dir, answers_dir, question_vocab, answer_vocab, images_dir, max_question_length, transform=None):\n",
        "        self.phase = phase\n",
        "        self.questions_json = questions_dir + \"/\" + self.phase + \".json\"\n",
        "        self.answers_json = answers_dir + \"/\" + self.phase + \".json\"\n",
        "        self.question_vocab_path = question_vocab_path\n",
        "        self.answer_vocab_path = answer_vocab_path\n",
        "        self.images_dir = images_dir\n",
        "        self.max_question_length = max_question_length\n",
        "        self.transform = transform\n",
        "\n",
        "        self.question_vocab = question_vocab\n",
        "        self.answer_vocab = answer_vocab\n",
        "\n",
        "        self.dataset = self.create_dataset()\n",
        "    \n",
        "\n",
        "    def create_dataset(self):\n",
        "        with open(self.questions_json) as f:\n",
        "            questions = json.load(f)[\"questions\"]\n",
        "        with open(self.answers_json) as f:\n",
        "            answers = json.load(f)[\"annotations\"]\n",
        "\n",
        "        dataset = []\n",
        "        file_loop = tqdm(enumerate(zip(questions, answers)), total=len(questions), colour=\"green\")\n",
        "        for idx, (q, a) in file_loop:\n",
        "            if(q[\"image_id\"]!=a[\"image_id\"]):\n",
        "                continue\n",
        "            image_id = str(q[\"image_id\"])\n",
        "            image_path = self.images_dir + \"/\" + self.phase + \"/\" + image_id + \".jpg\"\n",
        "\n",
        "            ans = a[\"answers\"]\n",
        "            answers = []\n",
        "\n",
        "            for answer in ans:\n",
        "                if((answer[\"answer_confidence\"]==\"yes\") and (answer[\"answer\"] not in answers)):\n",
        "                    answers.append(answer[\"answer\"])\n",
        "            \n",
        "            sample = {}\n",
        "            sample[\"image_path\"] = image_path\n",
        "            sample[\"question\"] = q[\"question\"]\n",
        "            sample[\"answers\"] = answers\n",
        "            dataset.append(sample)\n",
        "\n",
        "            file_loop.set_description(f\"Generating {self.phase} data\")\n",
        "        \n",
        "        random.shuffle(dataset)\n",
        "        return dataset\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        \n",
        "        sample = self.dataset[index]\n",
        "        image_path =  sample[\"image_path\"]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        question = np.array([self.question_vocab.word2idx(\"<pad>\")] * self.max_question_length)\n",
        "        question_tokenised = [self.question_vocab.word2idx(token) for token in tokenise(sample[\"question\"])]\n",
        "        if len(question_tokenised)<=self.max_question_length:\n",
        "            question[:len(question_tokenised)] = question_tokenised\n",
        "        else:\n",
        "            question = question_tokenised[:self.max_question_length]\n",
        "\n",
        "        all_answers = [self.answer_vocab.word2idx(ans) for ans in sample[\"answers\"]]\n",
        "        if self.answer_vocab.unk_index in all_answers:\n",
        "            all_answers.remove(self.answer_vocab.unk_index)\n",
        "        if len(all_answers)==0:\n",
        "            answer = self.answer_vocab.unk_index\n",
        "        else:\n",
        "            answer = random.choice(all_answers)\n",
        "        \n",
        "        return image, question, answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG8HLoD6MRg5"
      },
      "outputs": [],
      "source": [
        "def get_loader(question_dir, answer_dir, question_vocab_path, answer_vocab_path, images_dir, max_question_length=20, transform=None, shuffle=True, num_workers=2, batch_size=64):\n",
        "    data_loader = {}\n",
        "    question_vocab = Vocab(question_vocab_path)\n",
        "    answer_vocab = Vocab(answer_vocab_path)\n",
        "    for phase in [\"train\", \"val\"]:\n",
        "        dataset = VQADataset(phase, question_dir, answer_dir, question_vocab, answer_vocab, images_dir, max_question_length, transform)\n",
        "        phase_dataloader = DataLoader(dataset=dataset,\n",
        "                                      batch_size=batch_size, \n",
        "                                      shuffle=shuffle,\n",
        "                                      num_workers=num_workers)\n",
        "        data_loader[phase] = phase_dataloader\n",
        "    \n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOdnY8wxf9h6"
      },
      "outputs": [],
      "source": [
        "class ImgEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size):\n",
        "\n",
        "        \"\"\"\n",
        "            loads pre-trained ResNet model \n",
        "            generates the image features from the input image\n",
        "        \"\"\"\n",
        "\n",
        "        super(ImgEncoder, self).__init__()\n",
        "        self.cnn_network = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
        "        in_features = self.cnn_network.fc.in_features\n",
        "        self.cnn_network.fc = nn.Identity()\n",
        "        self.fc = nn.Linear(in_features, embed_size)\n",
        "        # self.tanh = nn.Tanh()   \n",
        "        \n",
        "        for parameter in self.cnn_network.parameters():\n",
        "            parameter.requires_grad = False \n",
        "\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"\n",
        "            Extract feature vector from image vector\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            img_feature = self.cnn_network(image).flatten(start_dim=1)  \n",
        "                \n",
        "        img_feature = F.normalize(img_feature, p=2.0, dim=1) \n",
        "        img_feature = self.fc(img_feature)   \n",
        "        # img_feature = self.tanh(img_feature)           \n",
        "\n",
        "        return img_feature\n",
        "\n",
        "\n",
        "class QstEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, qst_vocab_size, word_embed_size, embed_size=1024, num_layers=2, hidden_size=512):\n",
        "\n",
        "        super(QstEncoder, self).__init__()\n",
        "        self.word2vec = nn.Embedding(qst_vocab_size, word_embed_size)\n",
        "        self.lstm = nn.LSTM(word_embed_size, hidden_size, num_layers)\n",
        "        self.fc = nn.Linear(2*num_layers*hidden_size, embed_size)\n",
        "        # self.tanh = nn.Tanh()     \n",
        "\n",
        "\n",
        "    def forward(self, question):\n",
        "\n",
        "        qst_vec = self.word2vec(question)                                                   \n",
        "        qst_vec = qst_vec.transpose(0, 1) \n",
        "        _, (hidden, cell) = self.lstm(qst_vec)                                          \n",
        "        qst_feature = torch.cat((hidden, cell), 2)                  \n",
        "        qst_feature = qst_feature.transpose(0, 1)                     \n",
        "        qst_feature = qst_feature.reshape(qst_feature.size()[0], -1)  \n",
        "        qst_feature = self.fc(qst_feature)  \n",
        "        # qst_feature = self.tanh(qst_feature)                         \n",
        "        return qst_feature\n",
        "\n",
        "\n",
        "class VQAModel(nn.Module):\n",
        "\n",
        "    def __init__(self, qst_vocab_size, ans_vocab_size, word_embed_size, embed_size=1024, num_layers=2, hidden_size=512):\n",
        "\n",
        "        super(VQAModel, self).__init__()\n",
        "        self.img_encoder = ImgEncoder(embed_size)\n",
        "        self.qst_encoder = QstEncoder(qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc1 = nn.Linear(embed_size, ans_vocab_size)\n",
        "        self.fc2 = nn.Linear(ans_vocab_size, ans_vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, img, qst):\n",
        "\n",
        "        img_feature = self.img_encoder(img)                     \n",
        "        qst_feature = self.qst_encoder(qst)                   \n",
        "        combined_feature = torch.mul(img_feature, qst_feature)  \n",
        "        combined_feature = self.tanh(combined_feature)\n",
        "        combined_feature = self.dropout(combined_feature)\n",
        "        combined_feature = self.fc1(combined_feature)           \n",
        "        combined_feature = self.tanh(combined_feature)\n",
        "        combined_feature = self.dropout(combined_feature)\n",
        "        combined_feature = self.fc2(combined_feature) \n",
        "        # prediction = self.tanh(combined_feature)         \n",
        "        \n",
        "        return combined_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGy-GKH3spwu"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch, model_dir, model, optimizer=None, scheduler=None):\n",
        "    checkpoint = {}\n",
        "    checkpoint[\"model_state\"] = model.state_dict()\n",
        "\n",
        "    checkpoint[\"optimizer_state\"] = None\n",
        "    if optimizer is not None:\n",
        "        checkpoint[\"optimizer_state\"] = optimizer.state_dict()\n",
        "\n",
        "    checkpoint[\"scheduler_state\"] = None\n",
        "    if scheduler is not None:\n",
        "        checkpoint[\"scheduler_state\"] = scheduler.state_dict()\n",
        "\n",
        "    epoch_name = str(epoch)\n",
        "    file_path = model_dir + \"/epoch-\" + epoch_name + \".pth\"\n",
        "    torch.save(checkpoint, file_path)\n",
        "\n",
        "\n",
        "def load_checkpoint(model_file_path, model, optimizer=None, scheduler=None):\n",
        "    if(os.path.exists(model_file_path)):\n",
        "        checkpoint = torch.load(model_file_path)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        if checkpoint[\"optimizer_state\"] is not None and optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "        if checkpoint[\"scheduler_state\"] is not None and scheduler is not None:\n",
        "            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
        "        print(\"Checkpoint loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKuxlz_2CfyQ"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, device, num_epochs, data_loader, model_file_path, model_dir):\n",
        "    model = model.to(device)\n",
        "    \n",
        "    if model_file_path is not None:\n",
        "        load_checkpoint(model_file_path, model, optimizer, scheduler)\n",
        "    \n",
        "    saved_state_loss:float = 1e9\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            if phase==\"train\":\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "                correct:int = 0\n",
        "                incorrect:int = 0\n",
        "                total_loss:float = 0.0\n",
        "\n",
        "            enumerator = tqdm(enumerate(data_loader[phase]), total=len(data_loader[phase]), leave=True, colour=\"green\")\n",
        "            for batch_idx, (image, question, answer) in enumerator:\n",
        "                image = image.to(device)\n",
        "                question = question.to(device).long()\n",
        "                output = model(image, question)      \n",
        "                answer = answer.to(device)\n",
        "                \n",
        "                loss = criterion(output, answer)\n",
        "\n",
        "                if phase==\"train\":\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    if scheduler is not None:\n",
        "                        scheduler.step()\n",
        "                    \n",
        "                    enumerator.set_description(f\"Epoch : [{epoch+1}/{num_epochs}]\")\n",
        "                    enumerator.set_postfix(loss=loss.item())\n",
        "                else:\n",
        "                    total_loss += loss.item()\n",
        "                    _, prediction = torch.max(output, 1)\n",
        "                    correct += (prediction==answer).sum().item()\n",
        "                    incorrect += (prediction!=answer).sum().item()\n",
        "                    \n",
        "                    enumerator.set_description(\"Running validation set. Calculating accuracy...\")\n",
        "\n",
        "            if phase==\"train\":\n",
        "                save_checkpoint(epoch+1, model_dir, model, optimizer, scheduler)\n",
        "            else:\n",
        "                val_accuracy = ((correct) / (correct + incorrect))\n",
        "                average_loss = (total_loss / (correct + incorrect))\n",
        "                print(f\"\\nAverage loss on validation set : {average_loss:.3f} | Accuracy on validation set : {val_accuracy:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71XXI01ZElsJ"
      },
      "outputs": [],
      "source": [
        "# resizing images\n",
        "train_input_dir = \"/content/train2014\"\n",
        "val_input_dir = \"/content/val2014\"\n",
        "images_dir = \"/content/images\"\n",
        "image_size = (224, 224)\n",
        "resize_image_dataset(\"train\", train_input_dir, images_dir, image_size)\n",
        "resize_image_dataset(\"val\", val_input_dir, images_dir, image_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generating vocabulary\n",
        "vocab_dir = \"/content/vocab\"\n",
        "questions_json = \"/content/questions/train.json\"\n",
        "min_word_count = 3\n",
        "question_vocab_path, question_vocab_size = create_question_vocab(questions_json, vocab_dir, min_word_count)\n",
        "\n",
        "topN = 1000\n",
        "answers_json = \"/content/answers/train.json\"\n",
        "answer_vocab_path, answer_vocab_size = create_answer_vocab(answers_json, vocab_dir, topN)"
      ],
      "metadata": {
        "id": "H1F3EaqQXsyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating datasets and dataloaders\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "max_question_length = 30\n",
        "max_number_answers = 10\n",
        "num_workers = 2\n",
        "batch_size = 64\n",
        "shuffle = True\n",
        "question_dir = \"/content/questions\"\n",
        "answer_dir = \"/content/answers\"\n",
        "\n",
        "data_loader = get_loader(question_dir=question_dir,\n",
        "                         answer_dir=answer_dir,\n",
        "                         question_vocab_path=question_vocab_path, \n",
        "                         answer_vocab_path=answer_vocab_path, \n",
        "                         images_dir=images_dir, \n",
        "                         max_question_length=max_question_length,\n",
        "                         transform=transform, \n",
        "                         shuffle=shuffle, \n",
        "                         num_workers=num_workers,\n",
        "                         batch_size=batch_size)"
      ],
      "metadata": {
        "id": "B7u-EKqbXs1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the model\n",
        "model_dir = \"/content/model_parameters\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "embed_size = 1024\n",
        "word_embed_size = 100\n",
        "num_layers = 2\n",
        "hidden_size = 512\n",
        "num_epochs = 20\n",
        "learning_rate = 3e-4\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = VQAModel(embed_size=embed_size,\n",
        "                 qst_vocab_size=question_vocab_size,\n",
        "                 ans_vocab_size=answer_vocab_size,\n",
        "                 word_embed_size=word_embed_size,\n",
        "                 num_layers=num_layers,\n",
        "                 hidden_size=hidden_size)  "
      ],
      "metadata": {
        "id": "HPb_3CGOXs3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_file_path = None # replace by path to existing model weights\n",
        "\n",
        "load_checkpoint(model_file_path, model, optimizer, scheduler)"
      ],
      "metadata": {
        "id": "qTx8FhMyAdRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "parameters = []\n",
        "for parameter in model.parameters():\n",
        "    if parameter.requires_grad:\n",
        "        parameters.append(parameter)  \n",
        "\n",
        "T_max = num_epochs * batch_size\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(parameters, lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\n",
        "\n",
        "model_file_path = \"/content/model_parameters/epoch-09.pth\"\n",
        "\n",
        "train_model(model=model,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            device=device,\n",
        "            num_epochs=num_epochs,\n",
        "            data_loader=data_loader,\n",
        "            model_file_path=model_file_path,\n",
        "            model_dir=model_dir)"
      ],
      "metadata": {
        "id": "IDXHvk05gcIK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}