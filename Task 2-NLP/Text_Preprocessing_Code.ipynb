{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing using regular expressions (Regex)"
      ],
      "metadata": {
        "id": "nukzVmZkCsTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text1 = \"Eighty-seven miles to go, yet.  Onward!\"\n",
        "# [\\w]+, searches only for word characters\n",
        "# [\\w-]+ also includes - in the same word -> See the difference in output\n",
        "word = re.findall('[\\w]+',text1)\n",
        "word1 = re.findall('[\\w-]+', text1)\n",
        "# Using NLTK library-> Infact our regular expression tokenizer is better. It's not considering the punctuations as tokens, which is what we want\n",
        "word2 = word_tokenize(text1)\n",
        "print(word)\n",
        "print(word1)\n",
        "print(word2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNP1xP4WDANm",
        "outputId": "0c834598-8cf6-4bd6-8394-4ced64552d04"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Eighty', 'seven', 'miles', 'to', 'go', 'yet', 'Onward']\n",
            "['Eighty-seven', 'miles', 'to', 'go', 'yet', 'Onward']\n",
            "['Eighty-seven', 'miles', 'to', 'go', ',', 'yet', '.', 'Onward', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "z1X8VMm2omuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031b7ef5-3cd1-4c87-fdc8-d71e3865601c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Founded', 'in', '2002,', 'SpaceX’s', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi-planet', 'species', 'by', 'building', 'a', 'self-sustaining', 'city', 'on', 'Mars.', 'In', '2008,', 'SpaceX’s', 'Falcon', '1', 'became', 'the', 'first', 'privately', 'developed', 'liquid-fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'Earth.']\n",
            "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \\nspecies by building a self-sustaining city on Mars', 'In 2008, SpaceX’s Falcon 1 became the first privately developed \\nliquid-fuel launch vehicle to orbit the Earth.']\n",
            "['Founded', 'in', '2002', 'SpaceX', 's', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi', 'planet', 'species', 'by', 'building', 'a', 'self', 'sustaining', 'city', 'on', 'Mars', 'In', '2008', 'SpaceX', 's', 'Falcon', '1', 'became', 'the', 'first', 'privately', 'developed', 'liquid', 'fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'Earth']\n",
            "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \\nspecies by building a self-sustaining city on Mars', 'In 2008, SpaceX’s Falcon 1 became the first privately developed \\nliquid-fuel launch vehicle to orbit the Earth.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "\n",
        "# Level - 1\n",
        "# Using split(), you can use only one separator at a time.\n",
        "#  Punctuations to the right of word are added to the word, not good.\n",
        "# Default separator is a space \" \"\n",
        "a = text.split() \n",
        "print(a)\n",
        "\n",
        "# For sentence tokenization, separator is '. '\n",
        "b = text.split('. ')\n",
        "print(b)\n",
        "\n",
        "# Level-2\n",
        "# Word tokenization\n",
        "# Using regular expressions\n",
        "# \\w means any word character(letters numbers _)\n",
        "tokens = re.findall(\"[\\w]+\", text)\n",
        "print(tokens)\n",
        "\n",
        "#Sentence tokenization\n",
        "# Sentence to be splitted when you see ? ! .\n",
        "sentences = re.compile('[.!?] ').split(text)\n",
        "print(sentences)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the Natural Language Toolkit (NLTK) for tokenization"
      ],
      "metadata": {
        "id": "vsvcGjpuErKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Natural language toolkit\n",
        "# Tokenizers divides a string into substrings. White Space not considered a token.\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_Jy9t1Z6tlZ",
        "outputId": "63f49dea-6400-4bf4-fe28-2c2b709e9585"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "\n",
        "# Word tokenize using the library\n",
        "# Note that every punctuation is also considered a token.\n",
        "# Ex: ' , . are taken as tokens, we don't want this.\n",
        "word = word_tokenize(text)\n",
        "print(word)\n",
        "sent = sent_tokenize(text)\n",
        "print(sent)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oszXyZl5hnlt",
        "outputId": "999a2c12-aa34-4bcd-90e4-e8269fca2fee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Founded', 'in', '2002', ',', 'SpaceX', '’', 's', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi-planet', 'species', 'by', 'building', 'a', 'self-sustaining', 'city', 'on', 'Mars', '.', 'In', '2008', ',', 'SpaceX', '’', 's', 'Falcon', '1', 'became', 'the', 'first', 'privately', 'developed', 'liquid-fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'Earth', '.']\n",
            "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \\nspecies by building a self-sustaining city on Mars.', 'In 2008, SpaceX’s Falcon 1 became the first privately developed \\nliquid-fuel launch vehicle to orbit the Earth.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming using NLTK"
      ],
      "metadata": {
        "id": "UbthUaSEIa-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming is like the precursor to lemmatization \n",
        "# In stemming we just try to chop off suffixes of the word, such that resultant is the simple version of the word\n",
        "# Ex:  \n",
        "# Porter's Stemmer algorithm is a popular stemming algorithm.\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
        "# Note: the stemmed word need not always be a real word!\n",
        "# Below is a case of over-stemming, all three words are not related to each other, but all three have the same stem from the PS algorithm\n",
        "words2 = [\"universal\",\"universe\",\"university\"]\n",
        "# Below is a case of under-stemming, the three words should have a common stem word due to almost same meaning but they don't from the algorithm\n",
        "words3 = [\"alumnus\",\"almuni\",\"alumnae\"]\n",
        "ps = PorterStemmer()\n",
        "for w in words:\n",
        "  print(w,\" -> \", ps.stem(w))\n",
        "print(\"\\n\")\n",
        "for w in words2:\n",
        "  print(w,\"->\",ps.stem(w))\n",
        "print(\"\\n\")\n",
        "for w in words3:\n",
        "  print(w,\"->\",ps.stem(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5LmsW2aIaed",
        "outputId": "c81ba47e-151c-46d6-907a-0c79a06dafff"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program  ->  program\n",
            "programs  ->  program\n",
            "programmer  ->  programm\n",
            "programming  ->  program\n",
            "programmers  ->  programm\n",
            "\n",
            "\n",
            "universal -> univers\n",
            "universe -> univers\n",
            "university -> univers\n",
            "\n",
            "\n",
            "alumnus -> alumnu\n",
            "almuni -> almuni\n",
            "alumnae -> alumna\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatizing using NLTK"
      ],
      "metadata": {
        "id": "PEnVzQEuEz64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "# Converting a word to it's 'simple' form ->Links words with similar meaning to one word\n",
        "# rocks -> rock\n",
        "# better -> good\n",
        "# corpora -> corpus\n",
        "\n",
        "# Lemmatization using NLTK library\n",
        "# nltk.stem library is used to remove 'morphological affixes' from a word, to leave only the word stem.\n",
        "# Few difficulties -> ceil is not the stem of ceiling!\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# wnl is now an object pf WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "# WordNet is a lexical database of words-> basically thesaurus-like but also includes relations between words.\n",
        "# Example: hypernyms -> Y is a hypernym of X, if every X is a (kind of) Y. -> canine is a hypernym of dog.\n",
        "# arguments are the word and the part of speech it corresponds to,\n",
        "# n - noun\n",
        "# a - adjective\n",
        "# v - verb\n",
        "# r = adverb\n",
        "print(wnl.lemmatize(\"worse\",pos=\"a\"))\n",
        "\n",
        "text2 = \"I am building spaceships to go to places, that otherwise would be impossible to go.\"\n",
        "word = re.findall('[\\w]+', text2)\n",
        "# word is the tokenization of text2\n",
        "print(word)\n",
        "\n",
        "# Default pos = \"n\" (noun), you need to mention if it's something else.\n",
        "mod = []\n",
        "for w in word:\n",
        "  modw = wnl.lemmatize(w)\n",
        "  if(modw == w):\n",
        "    modw = wnl.lemmatize(w,pos=\"v\")\n",
        "  mod.append(modw)\n",
        "\n",
        "# word after lemmatization\n",
        "print(mod)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "710qV5Ob8Pb_",
        "outputId": "233917a8-6904-4c8d-9900-ed52d3a9ba0d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bad\n",
            "['I', 'am', 'building', 'spaceships', 'to', 'go', 'to', 'places', 'that', 'otherwise', 'would', 'be', 'impossible', 'to', 'go']\n",
            "['I', 'be', 'build', 'spaceship', 'to', 'go', 'to', 'place', 'that', 'otherwise', 'would', 'be', 'impossible', 'to', 'go']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings (also called word vectors)"
      ],
      "metadata": {
        "id": "qWbsl4LPQ0MM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Word embedding is basically associating a word with a vector, each dimension (or) column being a set of measurable characteristics of the word.\n",
        "\n",
        "2. The closer the vectors are in the 100 (or any other) dimension space, closer are their meanings.\n",
        "\n",
        "3. Word2vec is an algorithm used to obtain word embeddings (vector representation of words)\n",
        "4. Glove is an unsupervised learning algorithm to do the same task. "
      ],
      "metadata": {
        "id": "eMWP0HT4Q_uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Given a pre-trained word embeddings, finding closely related words."
      ],
      "metadata": {
        "id": "fbwXOf_pa7hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Generating word vectors using Word2Vec\n",
        "rev = pd.read_csv(\"Reviews.csv\",engine=\"python\", error_bad_lines=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwOhHvwNbJOD",
        "outputId": "8b886f1e-c8ab-4dd2-94c8-853633959dad"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "Skipping line 168205: unexpected end of data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use the Text column of the dataset\n",
        "# Preparing corpus\n",
        "# Taking 1000 rows, and appending each text column with new line.\n",
        "corpus = '\\n'.join(rev[:1000]['Text'])\n",
        "data = []\n",
        "for i in sent_tokenize(corpus):\n",
        "  temp = []\n",
        "  for j in word_tokenize(i):\n",
        "    temp.append(j.lower())\n",
        "\n",
        "  data.append(temp)\n",
        "\n",
        "# Implementing CBOW architecture\n",
        "model = gensim.models.Word2Vec(data, min_count = 1,size = 100, window = 5, sg=0)\n",
        "model.save(\"word2vec.model\")\n",
        "\n",
        "# model = Word2Vec.load(\"word2vec.model\") -> Reloading model later\n",
        "\n",
        "# mapping each word with a vector from the model above \n",
        "word_vectors = model.wv\n",
        "# Voila! Our word vectors. \n",
        "# print(word_vectors['eat'])\n",
        "similar = model.wv.most_similar('use', topn = 5)\n",
        "print(similar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znAJku0Pc52b",
        "outputId": "1f21332e-096d-417b-8b83-0454fa85f76b"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('who', 0.9999181032180786), ('add', 0.9998929500579834), ('hard', 0.9998905658721924), ('then', 0.9998830556869507), ('out', 0.9998829364776611)]\n"
          ]
        }
      ]
    }
  ]
}